{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/ai-tech-news-bot/blob/main/computer-vision/cook/image-classification/Image_classification_ConvNeXT_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image classification with ConvNEXT using a Hugging Face dataset.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The model is set at facebook/convnext-large-224 but could do just as well with facebook/convnext-tiny-224.\n",
        "\n",
        "Batch size is 32, epoch is 3.\n",
        "\n",
        "**Make sure you change the dataset to what you need.** My dataset I've used has both a training and a validation set, so change the code accordingly if you don't have a validation set."
      ],
      "metadata": {
        "id": "9FNYumvarvVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSL3DyktmTqF"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"ilsilfverskiold/traffic-camera-norway-images\" # public dataset (possible to import private too)\n",
        "model = \"facebook/convnext-large-224\" # decide on your model\n",
        "learning_rate = 5e-5\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "unB6J5BcvZiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dataset from huggingface below."
      ],
      "metadata": {
        "id": "u5FS_gigtJ1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsr6UVqgmfiu"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(dataset)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the features and get the labels. Make sure the images are in PIL format."
      ],
      "metadata": {
        "id": "8ENvHpSxtREB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zoa84Saqml86"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpxt_VP1m2BE"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS_nTcvlm7FR",
        "outputId": "407084fc-446f-4652-a133-f5a76e2b2219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['high-traffic', 'low-traffic', 'medium-traffic', 'no-traffic']\n"
          ]
        }
      ],
      "source": [
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmiDyQeWm8r0"
      },
      "outputs": [],
      "source": [
        "id2label = {k:v for k,v in enumerate(labels)}\n",
        "label2id = {v:k for k,v in enumerate(labels)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the dataset for fine-tuning with ViT/ConvNEXT/Swin Transformer we'll use an image prcoessor to normalize. The image processor ensures that every input image conforms to expectations (input image size and pixel value range)."
      ],
      "metadata": {
        "id": "nICYmYZutiOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A11_9o6-nDet"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is defining a set of image transformations that are applied to the training data. These transformations prepare images for input into a neural network by normalizing them and augmenting the dataset to improve model robustness."
      ],
      "metadata": {
        "id": "g0lc_ULJwIjC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uDIpxPHnHlA"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "     RandomResizedCrop(image_processor.size[\"shortest_edge\"]),\n",
        "     RandomHorizontalFlip(),\n",
        "     ToTensor(),\n",
        "     normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "def train_transforms(examples):\n",
        "  examples[\"pixel_values\"] = [transform(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy1AX5WUnKhP"
      },
      "outputs": [],
      "source": [
        "processed_dataset = dataset.with_transform(train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the collate_fn function below is to control how a list of samples (gathered from the dataset) is merged into a single batch. This function is crucial for ensuring that batches are structured properly before being fed into a model during training or evaluation."
      ],
      "metadata": {
        "id": "__3cAaOQwZG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xslfOfmnRi-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(examples):\n",
        "  pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "  labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "\n",
        "  return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "dataloader = DataLoader(processed_dataset[\"train\"], collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
        "dataloader_validation = DataLoader(processed_dataset[\"validation\"], collate_fn=collate_fn, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2m2tV6lnTom",
        "outputId": "a1681cac-3026-417c-a408-2bd4c8b748ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel_values torch.Size([4, 3, 224, 224])\n",
            "labels torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "batch = next(iter(dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the labels we set up earlier from the dataset when importing the pre-trained model below, we also tell it to ignore the pre-defined labels that it previously have been trained on."
      ],
      "metadata": {
        "id": "s_QI3PAbwxbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zvqs1k8nVzZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(model,\n",
        "                                                        id2label=id2label,\n",
        "                                                        label2id=label2id,\n",
        "                                                        ignore_mismatched_sizes=True) # set to true to ignore the pre-defined labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model, make sure you check the training loss/accuracy, you want one to consistently go down while accuracy should go up. If it fluctuates makes sure you check the performance of the model. I'm using other metrics like precision, recall and f1 too but accuracy is usually the most important of them."
      ],
      "metadata": {
        "id": "rrIPI2b2xHdY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfUJpbsmnfA-"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    model.train()\n",
        "    training_predictions = []\n",
        "    training_labels = []\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = logits.argmax(-1)\n",
        "        training_predictions.extend(preds.cpu().numpy())\n",
        "        training_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    # Calculate training metrics (you can remove some of these if you only need accuracy or so)\n",
        "    train_accuracy = accuracy_score(training_labels, training_predictions)\n",
        "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(training_labels, training_predictions, average='weighted')\n",
        "\n",
        "    print(f\"\\nTraining Loss: {loss.item()}\")\n",
        "    print(f\"Training Accuracy: {train_accuracy}\")\n",
        "    print(f\"Training Precision: {train_precision}\")\n",
        "    print(f\"Training Recall: {train_recall}\")\n",
        "    print(f\"Training F1 Score: {train_f1}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        validation_predictions = []\n",
        "        validation_labels = []\n",
        "\n",
        "        for batch in tqdm(dataloader_validation):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
        "            logits = outputs.logits\n",
        "\n",
        "            preds = logits.argmax(-1)\n",
        "            validation_predictions.extend(preds.cpu().numpy())\n",
        "            validation_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "        val_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
        "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(validation_labels, validation_predictions, average='weighted')\n",
        "\n",
        "        print(f\"\\nValidation Accuracy: {val_accuracy}\")\n",
        "        print(f\"Validation Precision: {val_precision}\")\n",
        "        print(f\"Validation Recall: {val_recall}\")\n",
        "        print(f\"Validation F1 Score: {val_f1}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model below so we can du inference on it. Sometimes you can have good metrics but the model doesn't perform well on new data, so check both."
      ],
      "metadata": {
        "id": "GknWIxm8yHX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym_eaVicnuvu"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"trained_model\")\n",
        "model.config.save_pretrained(\"trained_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tWGKfSlpk4I"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\"trained_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8GIbEEfyxO3"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-classification\", model=model, image_processor=image_processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I usually mount my Google Drive to use new images to test with. This is not necessary if you want to test it in Hugging Face after you've deployed it."
      ],
      "metadata": {
        "id": "OQyVTemuyU3E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3EO6TvLqQ7I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sEGFJKeqTOB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = '/content/drive/MyDrive/your_image_you_want_to_test.png'\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image\n",
        "\n",
        "results = pipe(image)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also check the validation set and loop through a few of them to see how they do."
      ],
      "metadata": {
        "id": "2tjLxVbQywTV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4agY8vnD9LM"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "for i in range(100):\n",
        "    image_data = dataset['validation'][i]['image']\n",
        "    label_index = dataset['validation'][i]['label']\n",
        "\n",
        "    if not isinstance(image_data, Image.Image):\n",
        "        image = Image.open(image_data)\n",
        "    else:\n",
        "        image = image_data\n",
        "\n",
        "    results = pipe(image)\n",
        "\n",
        "    print(f\"Results for image {i+1}:\")\n",
        "    print(results)\n",
        "    print(\"Actual label:\", id2label[label_index])\n",
        "    print(\"----------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're ready to go we can push it to Hugging Face. You'll need a token that has both read/write rights that you find under Settings in your Hugging Face account."
      ],
      "metadata": {
        "id": "dfYNNBany4i9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aha-ou0I0Hcl"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iAK6Ulrz6Kk"
      },
      "outputs": [],
      "source": [
        "repo_name = \"\"\n",
        "\n",
        "model.push_to_hub(repo_name)\n",
        "image_processor.push_to_hub(repo_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO72vwQ4r8lG1wg55E8Wz1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}